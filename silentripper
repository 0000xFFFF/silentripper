#!/usr/bin/env python
import os
import subprocess
import time
import argparse
from pathlib import Path
import concurrent.futures
import threading

# Argument parsing
parser = argparse.ArgumentParser(description='Remove silent parts from video using FFmpeg')
parser.add_argument('-d', '--duration', metavar='sec', type=float, default=1, help="Silence duration in seconds (default: 1, min: 1)")
parser.add_argument('-m', '--min_duration', metavar='sec', type=float, default=1, help="Minimum duration for each sounded clip in seconds (default: 0)")
parser.add_argument('-n', '--noise', metavar='dB', type=int, default=-40, help="Noise level in dB (default: -40)")
parser.add_argument('-c', '--copy', action='store_true', help="Use copy codec for faster but potentially glitchy output")
parser.add_argument('-p', '--pause', action='store_true', help="Prompt before each action")
parser.add_argument('-t', '--threads', metavar='N', type=int, default=4, help="Number of worker threads")
parser.add_argument('-o', '--output', metavar='file', type=str, help="Output filename (default: \"filename} (cut).{ext}\")")
parser.add_argument('-g', '--gpu', action='store_true', help="Tell FFmpeg to use GPU (AMD is by default edit the script to use something else)")
parser.add_argument('filename', type=argparse.FileType('r'))
args = parser.parse_args()

program_start_time = time.time()

# File details
input_file_path = Path(args.filename.name)
file_name = input_file_path.stem
file_extension = input_file_path.suffix
full_file_path = input_file_path.absolute()
output_file_name = args.output or f"{file_name} (cut){file_extension}"

# Global variables
altered_clips = 0
total_muted_duration = 0
total_sounded_duration = 0
failed_conversions = 0

# Get total video duration
def get_video_duration(file_path):
    ffprobe_cmd = [
        'ffprobe', '-v', 'error', '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1', file_path
    ]
    result = subprocess.run(ffprobe_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return float(result.stdout.strip())

total_duration = get_video_duration(full_file_path)

# Print initial settings
print(f"Noise level.............: {args.noise} dB")
print(f"Silence duration........: {args.duration} s")
print(f"Minimum sounded duration: {args.min_duration} s")
print(f"Copy codec..............: {args.copy}")
print(f"GPU.....................: {args.gpu}")
print(f"Threads.................: {args.threads}")
print(f"Total video duration....: {total_duration:.2f} s")
print(f"Output file.............: {output_file_name}")

# Run FFmpeg to detect silent sections
ffmpeg_proc = subprocess.Popen(
    [
        'ffmpeg', '-hide_banner', '-vn', '-i', str(full_file_path),
        '-af', f'silencedetect=n={args.noise}dB:d={args.duration}', '-f', 'null', '-'
    ],
    stdout=subprocess.PIPE, stderr=subprocess.STDOUT
)

# Parse FFmpeg output for silence start/end timestamps
muted_sections = []
for line in ffmpeg_proc.stdout.read().decode().split("\n"):
    line = line.strip()
    if "silencedetect" not in line:
        continue
    if "silence_start: " in line:
        start_time = line.split("silence_start: ")[1]
    elif "silence_end: " in line:
        end_time, duration = line.split("silence_end: ")[1].split(" | silence_duration: ")
        muted_sections.append([start_time, end_time, duration])

# Print muted sections
print("===[ MUTED TIMESTAMPS (START, END, DURATION) ]===")
for idx, (start, end, duration) in enumerate(muted_sections, start=1):
    total_muted_duration += float(duration)
    print(f"{idx}. Start: {start}, End: {end}, Duration: {duration} seconds")

sounded_clips = []  # will hold dicts
def add_sounded_clip(start, end):
    global altered_clips
    output_file = f"clip_{len(sounded_clips)+1}.mp4"

    if end is None:
        end = str(total_duration)

    duration = round(float(end) - float(start), 5)
    if duration < args.min_duration:
        altered_clips += 1
        end = str(round(float(end) + args.min_duration - duration, 5))
        duration = round(float(end) - float(start), 5)

    sounded_clips.append({
        "start": start,
        "end": end,
        "duration": duration,
        "file": output_file,
        "success": False
    })

# Add clips for sounded parts
if muted_sections:
    # Before first silence
    first_mute_start = muted_sections[0][0]
    add_sounded_clip("0", first_mute_start)

    # Between silences
    for i in range(len(muted_sections)):
        start = muted_sections[i][1]
        end = muted_sections[i+1][0] if i+1 < len(muted_sections) else None
        add_sounded_clip(start, end)

# Print sounded sections
print("===[ SOUNDED TIMESTAMPS (OUTPUT FILE, START, END, DURATION) ]===")
for idx, clip in enumerate(sounded_clips, start=1):
    total_sounded_duration += float(clip["duration"])
    print(f"{idx}. Output: {clip["file"]}, Start: {clip["start"]}, End: {clip["end"]}, Duration: {clip["duration"]} seconds")

# Convert seconds to HH:MM:SS
def format_time(seconds):
    return time.strftime("%H:%M:%S", time.gmtime(seconds))

# Summary
print(f"\nMuted sections: {len(muted_sections)}")
print(f"Sounded sections: {len(sounded_clips)}")
print(f"Altered clips: {altered_clips}")
print(f"Total muted duration: {format_time(total_muted_duration)} ({total_muted_duration:.2f} seconds)")
print(f"Total sounded duration: {format_time(total_sounded_duration)} ({total_sounded_duration:.2f} seconds)\n")

# Pause if necessary
if args.pause:
    try:
        input("Press Enter to start processing...")
    except:
        exit()

print_mutex = threading.Lock()

# Function to process a single clip
def process_clip(clip, i, total):

    ffmpeg_cmd = ['ffmpeg', '-y']

    # Input options (before -i)
    if args.gpu:
        ffmpeg_cmd += ['-hwaccel', 'vaapi', '-vaapi_device', '/dev/dri/renderD128']
    else:
        if args.copy:
            ffmpeg_cmd += ['-c', 'copy']

    # Input file
    ffmpeg_cmd += ['-ss', clip["start"], '-to', clip["end"], '-i', str(full_file_path)]

    # Output options (after -i)
    if args.gpu:
        ffmpeg_cmd += ['-vf', 'format=nv12,hwupload', '-c:v', 'h264_vaapi']

    ffmpeg_cmd += [clip["file"]]

    with print_mutex:
        print(f"{i}/{total} {clip['file']}: {clip['start']} -- {clip['end']} ({clip['duration']})")

    start_time = time.time()
    #result = subprocess.run(ffmpeg_cmd)
    result = subprocess.run(ffmpeg_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    elapsed = time.time() - start_time

    if result.returncode == 0:
        with print_mutex:
            print(f"{i}/{total} {clip['file']}: {clip['start']} -- {clip['end']} ({clip['duration']}) ok (took: {elapsed:.2f}s)")
        return clip["file"], True
    else:
        with print_mutex:
            print(f"{i}/{total} {clip['file']}: {clip['start']} -- {clip['end']} ({clip['duration']}) fail")
        return clip["file"], False

num_workers = args.threads

lock = threading.Lock()

# Process clips in parallel
with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
    total = len(sounded_clips)
    futures = {executor.submit(process_clip, clip, i, total): clip for i, clip in enumerate(sounded_clips, start=1)}
    for future in concurrent.futures.as_completed(futures):
        output_file, success = future.result()
        with lock:
            for c in sounded_clips:
                if c["file"] == output_file:
                    c["success"] = success
                    if not success:
                        failed_conversions += 1
                    break


with open("clip_list.txt", "w") as clip_list_file:
    for clip in sounded_clips:
        if clip["success"]:
            clip_list_file.write(f"file '{clip['file']}'\n")

print(f"\nFailed conversions: {failed_conversions}")

if args.pause:
    try:
        input("Press Enter to concat videos...")
    except:
        exit()

# Concatenate sounded clips into final output
print("===[ CONCAT ]===")
concat_cmd = [
    'ffmpeg', '-hide_banner', '-loglevel', 'error',
    '-f', 'concat', '-safe', '0',
    '-y'
]

if args.copy:
    concat_cmd += ['-c', 'copy']
elif args.gpu:
    concat_cmd += ['-hwaccel', 'vaapi', '-vaapi_device', '/dev/dri/renderD128']

concat_cmd += ['-i', 'clip_list.txt']

# Output options (after -i)
if args.gpu:
    # Example: VAAPI on Linux
    concat_cmd += ['-vf', 'format=nv12,hwupload', '-c:v', 'h264_vaapi']
    # Example: AMF on Windows (switch if needed)
    # concat_cmd += ['-c:v', 'h264_amf']

concat_cmd += [output_file_name]

print(f"\nConcatenating clips into: {output_file_name}")
result = subprocess.run(concat_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
if result.returncode != 0:
    print(f"Concatenation failed with error: {result.stderr.decode('utf-8')}")
else:
    print("Concatenation succeeded!")

# Pause if necessary before cleanup
if args.pause:
    try:
        input("Press Enter to clean up temporary files...")
    except:
        exit()

# Cleanup temporary files
for clip in sounded_clips:
    try:
        os.remove(clip["file"])
    except:
        pass
try:
    os.remove("clip_list.txt")
except:
    pass

print("Cleanup complete.")
print(f"Took: {time.time() - program_start_time:.2f}")
